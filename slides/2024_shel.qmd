---
title: Large Language Models
subtitle: How we got here and how that history can help us evaluate what they produce
format: 
  clean-revealjs:
    transition: fade
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: David Brown
    orcid: 0000-0001-7745-6354
    email: dwb2@andrew.cmu.edu
    affiliations: Studies in the History of the Language-13
date: October 19, 2024
title-slide-attributes: 
  data-background-image: img/img_2024_shel/2024_shel_qr.svg
  data-background-size: 15%
  data-background-position: 75% 70%
bibliography: refs/refs_2024_shel.bib
---

# Overview {background-color="#40666e"}

## Overview

### How did we get to large language models (LLMs)

-   Our topics
    -   Review some history of natural language processing (NLP) and digital writing technologies
    -   Look at the architectures of models and how they've changed over time
    -   Share the results of some research by our team and discuss some of the potential implications

## Overview

### How did we get to large language models (LLMs)

-   Our goals
    -   Show how these technologies fundamentally work.
    -   Highlight the affordances and limitations of LLMs and similar technologies.

# History {background-color="#40666e"}

## History

### Writing is inseparable from technological change

::: {style="font-size: 75%;"}
In 2009, researchers at Google published an article that coincided with the release of its N-gram Viewer and the corresponding data tables [@halevy2009unreasonable].
:::

::: {style="font-size: 90%;"}
> *But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus—along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks—[if only we knew how to extract the model from the data]{.bg style="--col: #f5b2c6"}.*
:::

::: {style="font-size: 75%; color: #636363;"}
We will return to this excerpt, but for now, let's focus on this final claim...
:::

## History

### The concept of a language model has been around for a long time…

-   speech recognition [@bahl1983maximum; @jelinek1985realtime]
-   spelling correction [@mays1991context]
-   machine translation [@brown1990statistical]

## History {auto-animate="true" auto-animate-easing="ease-in-out"}

### The concept of a language model has been around for a long time…

-   machine translation [@brown1990statistical]

\

:::: r-stack
::: {data-id="time1" auto-animate-delay="0" style="background-image: url('img/img_2024_shel/mt_timeline_01.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 200px; background-position: center;"}
:::
::::

## History {auto-animate="true" auto-animate-easing="ease-in-out"}

### The concept of a language model has been around for a long time…

-   machine translation [@brown1990statistical]

\

:::: r-stack
::: {data-id="time1" auto-animate-delay="0" style="background-image: url('img/img_2024_shel/mt_timeline_01.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 400px; background-position: left;"}
:::
::::

## History {auto-animate="true" auto-animate-easing="ease-in-out"}

### The concept of a language model has been around for a long time…

![](img/img_2024_shel/mt_timeline_02.png)

:::::: r-hstack
::: {data-id="box1" auto-animate-delay="0" style="background: #d98b19; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;"}
1960-1980

Beginnings of NLP
:::

::: {data-id="box2" auto-animate-delay="0.1" style="background: #b71848; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;"}
1980-2015

Towards Computation
:::

::: {data-id="box3" auto-animate-delay="0.2" style="background: #40666e; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;"}
2015-

Emergence of ML
:::
::::::

## History {auto-animate="true" auto-animate-easing="ease-in-out"}

### The concept of a language model has been around for a long time…

![](img/img_2024_shel/mt_timeline_02.png)

:::::: r-hstack
::: {data-id="box1" style="background: #d98b19; width: 375px; height: 10px; margin: 10px;"}
:::

::: {data-id="box2" style="background: #b71848; width: 475px; height: 10px; margin: 10px;"}
:::

::: {data-id="box3" style="background: #40666e; width: 150px; height: 10px; margin: 10px;"}
:::
::::::

# [The beginnings of NLP]{style="color: white;"} {background-color="#d98b19"}

## The beginnings of NLP

### The question of multiple meanings (or polysemy)

:::::: columns
::: {.column width="60%"}
![](img/img_2024_shel/weaver_memo_01.png)
:::

:::: {.column width="40%"}
::: {style="font-size: 75%; padding-top: 25px;"}
> A memo shared with a small group of researchers who were at the forefront of machine translation after WWII, anticipates the challenges and possibilities of the computer analysis of text. [@weaver1949translation]
:::
::::
::::::

## The beginnings of NLP

### The question of multiple meanings (or polysemy)

:::::: columns
::: {.column width="50%"}
![](img/img_2024_shel/weaver_memo_02.png)
:::

:::: {.column width="50%"}
::: {style="font-size: 75%; padding-top: 25px;"}
> *If one examines the words in a book, one at a time as through an opaque mask with a hole in it on word wide, then it is obviously impossible to determine, one at a time, the meaning of words.*
:::
::::
::::::

## The beginnings of NLP

### The question of multiple meanings (or polysemy)

:::::: columns
::: {.column width="50%"}
![](img/img_2024_shel/weaver_memo_02.png)
:::

:::: {.column width="50%"}
::: {style="font-size: 75%; padding-top: 25px;"}
> *But if one lengthens the slit in the opaque mask, until one can see not only the central word in question, but also say [a] N[umber of] words on either side, then if [that] N[umber] is large enough one can unambiguously decide the meaning of the central word.*
:::
::::
::::::

## The beginnings of NLP

### The question of multiple meanings (or polysemy)

:::::: columns
::: {.column width="50%"}
![](img/img_2024_shel/weaver_memo_02.png)
:::

:::: {.column width="50%"}
::: {style="font-size: 75%; padding-top: 25px;"}
> *The practical question is, what minimum value of N will, at least in a tolerable fraction of cases, lead to the correct choice of meaning for the central word?*
:::
::::
::::::

## The beginnings of NLP

### The question of multiple meanings (or polysemy)

-   "You shall know a word by the company it keeps." [@firth1957papers]
-   The meaning of word can be determined by examining the contextual window or span around that word.

## The beginnings of NLP

### The question of multiple meanings (or polysemy)

\

|                    Preceding | Word | Following                                          |
|--------------------:|:---:|-----------------------------------|
|     upscaling generally hold | fast | during a 4K 60FPS gaming session.                  |
|            a dragster, going | fast | in a straight line is actually pretty boring       |
|          The benefits of the | fast | can be maintained long term,                       |
| adopted slowly, but comes on | fast | once it's hit the mainstream                       |
|  They simply disagree on how | fast | to go and how best to get there in superseding it. |
|         which appeared stuck | fast | in the ground it had plowed up                     |

## The beginnings of NLP

### The question of multiple meanings (or polysemy)

\

|                                                                         Preceding | Word | Following                                                                                               |
|--------------------:|:---:|-----------------------------------|
|     upscaling generally [hold]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [during]{style="background-color: #f5b2c6; opacity: 0.75;"} a 4K 60FPS gaming session.                  |
|            a dragster, [going]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [in]{style="background-color: #f5b2c6; opacity: 0.75;"} a straight line is actually pretty boring       |
|          The benefits of [the]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [can]{style="background-color: #f5b2c6; opacity: 0.75;"} be maintained long term,                       |
| adopted slowly, but comes [on]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [once]{style="background-color: #f5b2c6; opacity: 0.75;"} it's hit the mainstream                       |
|  They simply disagree on [how]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [to]{style="background-color: #f5b2c6; opacity: 0.75;"} go and how best to get there in superseding it. |
|         which appeared [stuck]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [in]{style="background-color: #f5b2c6; opacity: 0.75;"} the ground it had plowed up                     |


## The beginnings of NLP

### The question of multiple meanings (or polysemy)

\

|                                                                         Preceding | Word | Following                                                                                               |
|--------------------:|:---:|-----------------------------------|
|     upscaling [generally hold]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [during a]{style="background-color: #f5b2c6; opacity: 0.75;"} 4K 60FPS gaming session.                  |
|            a [dragster, going]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [in a]{style="background-color: #f5b2c6; opacity: 0.75;"} straight line is actually pretty boring       |
|          The benefits [of the]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [can be]{style="background-color: #f5b2c6; opacity: 0.75;"} maintained long term,                       |
| adopted slowly, but [comes on]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [once it's]{style="background-color: #f5b2c6; opacity: 0.75;"} hit the mainstream                       |
|  They simply disagree [on how]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [to go]{style="background-color: #f5b2c6; opacity: 0.75;"} and how best to get there in superseding it. |
|         which [appeared stuck]{style="background-color: #f5b2c6; opacity: 0.75;"} | fast | [in the]{style="background-color: #f5b2c6; opacity: 0.75;"} ground it had plowed up                     |


## The beginnings of NLP

-   The ["context window"]{.bg style="--col: #f5b2c6"} is a fundamental insight that powers the training of LLMs (from word2vec to BERT to ChatGPT).

\

![](img/img_2024_shel/context_window.png)

## The beginnings of NLP

### As early as the mid-twentieth century, researchers…

-   had considered the potential for a "context window" to solve word-sense disambiguation

-   were developing the statistical tools that would eventually power the training of LLMs (i.e., neural networks).

::: callout-warning
## Question

Why didn’t we have LLMs sooner?
:::

## The beginnings of NLP

::: callout-warning
## Question

Why didn’t we have LLMs sooner?
:::

-   limited computational power
-   lack of training data
-   ascendance of universal grammar

## The beginnings of NLP

### Context free grammar

-   To cope with these limitations (and beliefs about language structure) early models resorted to hard-coding rules

::: {style="font-size: 75%; font-family: monospace; text-indent: 30%;"}
S → NP VP

NP → the N

VP → V NP

V → sings \| eats

N → cat \| song \| canary

-- the canary sings the song

-- the song eats the cat
:::

## The beginnings of NLP

### Context free grammar

:::::: columns
::: {.column width="50%"}
![](img/img_2024_shel/alpac_01.png)
:::

:::: {.column width="50%"}
::: {style="font-size: 75%; padding-top: 25px;"}
> The ALPAC Report, which was released in 1966, was highly skeptical of these kinds of approaches.
:::
::::
::::::

## The beginnings of NLP

### Context free grammar

:::::: columns
::: {.column width="50%"}
![](img/img_2024_shel/alpac_02.png)
:::

:::: {.column width="50%"}
::: {style="font-size: 75%; padding-top: 25px;"}
> *…we do not have useful machine translation. Furthermore, there is no immediate or predictable prospect of useful machine translation.*
:::
::::
::::::

## The beginnings of NLP

### Context free grammar

:::::: columns
::: {.column width="50%"}
![](img/img_2024_shel/alpac_03.png)
:::

:::: {.column width="50%"}
::: {style="font-size: 75%; padding-top: 25px;"}
> *Some of the work must be done on a rather large scale, since small-scale experiments and work with miniature models of language have proved seriously deceptive in the past, and one can come to grips with real problems only above a certain scale of grammar size, dictionary size, and available corpus.*
:::
::::
::::::

# [Towards computation]{style="color: white;"} {background-color="#b71848"}

## Towards computation

### Converting words into numbers

\

:::::: columns
:::: {.column width="40%"}
::: {style="font-size: 75%; padding-top: 25px;"}
-   The make-up or sampling frame of the Brown family of corpora. [@kucera1967computational]
-   From the 15 categories, 2000-word text samples were selected.
-   2000 x 500 ≈ 1,000,000 words
:::
::::

::: {.column width="60%"}
```{r}
#| echo: false

readr::read_csv("data/data_2024_shel/brown_sampling_frame.csv") |>
  knitr::kable("html") |>
  kableExtra::kable_styling(font_size = 18)
```
:::
::::::

## Towards computation

### A document-feature matrix (or a document-term matrix)

```{r}
#| echo: false
#| tbl-cap: "Absolute frequency in the Brown Corpus."

readr::read_csv("data/data_2024_shel/brown_dtm.csv", na = "NA") |> 
  knitr::kable("html", align = "r", table.attr = "style='width:90%;'") |>
  kableExtra::kable_styling(font_size = 18)
```

## Towards computation

### A document-feature matrix (or a document-term matrix)

```{r}
#| echo: false
#| tbl-cap: "Observations."

readr::read_csv("data/data_2024_shel/brown_dtm.csv", na = "NA") |> 
  knitr::kable("html", align = "r", table.attr = "style='width:90%;'") |>
  kableExtra::kable_styling(font_size = 18) |>
  kableExtra::column_spec(1, background = "#f5b2c6")
```

## Towards computation

### A document-feature matrix (or a document-term matrix)

```{r}
#| echo: false
#| tbl-cap: "Variables."

readr::read_csv("data/data_2024_shel/brown_dtm.csv", na = "NA") |> 
  knitr::kable("html", align = "r", table.attr = "style='width:90%;'") |>
  kableExtra::kable_styling(font_size = 18) |>
  kableExtra::row_spec(0, background = "#f5b2c6")
```

## Towards computation {auto-animate="true" auto-animate-easing="ease-in-out"}

### Zipf's Law

:::::::: r-vstack
::: {data-id="box0" auto-animate-delay="0" style="background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;"}
Most frequent words:
:::

::: {data-id="box1" auto-animate-delay="0" style="background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;"}
the
:::

::: {data-id="box2" auto-animate-delay="0" style="background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle"}
of
:::

::: {data-id="box3" auto-animate-delay="0" style="background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle"}
and
:::

::: {data-id="scatter1" auto-animate-delay="0" style="background-image: url('img/img_2024_shel/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 800px; height: 400px; background-position: left;"}
:::
::::::::

## Towards computation {auto-animate="true" auto-animate-easing="ease-in-out"}

### Zipf's Law

::::::: r-stack
::: {data-id="scatter1" auto-animate-delay="0" style="background-image: url('img/img_2024_shel/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 800px; height: 400px; background-position: left;"}
:::

::: {data-id="box1" auto-animate-delay="0" style="background: #d98b19; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 18%; left: 20.5%; opacity: 0.5;"}
:::

::: {data-id="box2" auto-animate-delay="0" style="background: #b71848; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 40%; left: 20.5%; opacity: 0.5;"}
:::

::: {data-id="box3" auto-animate-delay="0" style="background: #40666e; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 45%; left: 20.5%; opacity: 0.5;"}
:::
:::::::

## Towards computation {auto-animate="true" auto-animate-easing="ease-in-out"}

### Zipf's Law

::::: r-stack
::: {data-id="scatter1" auto-animate-delay="0" style="background-image: url('img/img_2024_shel/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 600px; background-position: left;"}
:::

::: {data-id="box0" auto-animate-delay="0" style="background: white; width: 500px; height: 200px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle; position: absolute; top: 18%; left: 35%;"}
-   Zipf's Law: the frequency of a token is inversely proportional to its rank.
-   Most tokens are infrequent.
:::
:::::


# Emergence of machine learning {background-color="#40666e"}

## Emergence of ML

### Let's return to the excerpt from the Google researchers.

::: {style="font-size: 90%;"}
> *But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus—along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks—if only we knew how to extract the model from the data.*
:::

::: callout-warning
## Question

What developments are taking place at this time (the early 2000s)?
:::

## Emergence of ML

::: callout-warning
## Question

What developments are taking place at this time (the early 2000s)?
:::

-   Expansion of the Internet.
-   Advances in memory and processing.
-   Oh & Jung publish “GPU implementation of neural networks” in Pattern Recognition. [-@oh2004gpu]

## Emergence of ML

-   Word2vec is released. [@mikolov2013efficient]
    -   Shallow (2-layer) neural network.
    -   Trained using a relatively small context window (\~10-12 words).
    -   Introduces "embeddings".

## Emergence of ML

### Embeddings from a vector model.

```{r}
#| echo: false
#| tbl-cap: "Embedding space."

readr::read_csv("data/data_2024_shel/vector_embeddings.csv", na = "NA") |> 
  knitr::kable("html", align = "r", table.attr = "style='width:90%;'") |>
  kableExtra::kable_styling(font_size = 14)
```

## Emergence of ML

### Embeddings from a vector model.

```{r}
#| echo: false
#| tbl-cap: "Dimensions."

readr::read_csv("data/data_2024_shel/vector_embeddings.csv", na = "NA") |> 
  knitr::kable("html", align = "r", table.attr = "style='width:90%;'") |>
  kableExtra::kable_styling(font_size = 14) |>
  kableExtra::row_spec(0, background = "#f5b2c6")
```

## Emergence of ML {background-image="img/img_2024_shel/cos_similarity_02.png" background-size="60%" background-opacity=".5"}

### Embeddings from a vector model.

-   When treated as coordinates in space, embeddings locate words that tend to appear together or in similar contexts near each other.

## Emergence of ML

### Embeddings from a vector model.

-   The proximity of words can be assessed using measures like cosine similarity.

$$
cosine~similarity = S_{c}(A, B) := cos(\theta) = \frac{A \cdot B}{||A||~||B||}
$$ ![](img/img_2024_shel/cos_similarity.png)

## Emergence of ML {background-video="img/img_2024_shel/vector_model_01.mp4" background-video-loop="true" background-video-muted="true"}

-   [An example of a vector model rendered in 3 dimensions]{style="color: white; background-color: #e64173; opacity: 0.75;"} from <https://projector.tensorflow.org/>.

## Emergence of ML {background-image="img/img_2024_shel/vector_model_02.png"}

### Embeddings from a vector model.

:::::: columns
:::: {.column width="25%"}
::: {style="font-size: 18px; padding-top: 10px;"}
|     token | similarity |
|----------:|-----------:|
|      slow |      0.448 |
|     quick |      0.519 |
|    faster |      0.568 |
|    slower |      0.593 |
|     speed |      0.602 |
|      busy |      0.646 |
|    simple |      0.663 |
|      food |      0.676 |
|    speeds |      0.688 |
|   fastest |      0.688 |
|      pace |      0.697 |
| efficient |      0.703 |
|      easy |      0.707 |
|     small |      0.710 |
|       too |      0.712 |
|  straight |      0.717 |
|     rapid |      0.717 |
|       low |      0.718 |
|   quickly |      0.718 |
|    packet |      0.718 |

: Tokens closest to *fast*
:::
::::

::: {.column width="75%"}
:::
::::::

## Emergence of ML

::: {style="font-size: 70%;"}
-   After the introduction of vector representations and, a short time later, the transformer architecture [@vaswani2017attention], language models have rapidly evolved. They can be grouped into roughly 3 generations.
:::

![](img/img_2024_shel/llms_generations.png)

## Emergence of ML

### Advances in LLMs…

-   Allowing for out-of-vocabulary words (using sub-words or word-pieces for tokenizing).

-   Adding a sequence layer.

-   Sliding a context window both left-to-right and right-to-left.

-   Implementing self-attention architecture.

-   Training on more and more data.

-   Expanding the context window (from 512 word-pieces for BERT to 128,000 word-pieces for GPT-4 Turbo 128K).

-   Introducing reinforcement learning from human feedback (RLHF) with Instruct GPT.

## Emergence of ML

### An example of contextual embeddings using BERT

::: {style="font-size: 80%;"}
```{python}
#| eval: false
#| echo: true
#| code-overflow: wrap

sentences = ["bank",
	"He eventually sold the shares back to the bank at a premium.",
	"The bank strongly resisted cutting interest rates.",
	"The bank will supply and buy back foreign currency.",
	"The bank is pressing us for repayment of the loan.",
	"The bank left its lending rates unchanged.",
	"The river flowed over the bank.",
	"Tall, luxuriant plants grew along the river bank.",
	"His soldiers were arrayed along the river bank.",
	"Wild flowers adorned the river bank.",
	"Two fox cubs romped playfully on the river bank.",
	"The jewels were kept in a bank vault.",
	"You can stow your jewelry away in the bank.",
	"Most of the money was in storage in bank vaults.",
	"The diamonds are shut away in a bank vault somewhere.",
	"Thieves broke into the bank vault.",
	"Can I bank on your support?",
	"You can bank on him to hand you a reasonable bill for your services.",
	"Don't bank on your friends to help you out of trouble.",
	"You can bank on me when you need money.",
	"I bank on your help."]

```
:::

## Emergence of ML

### An example of contextual embeddings using BERT

::: {style="font-size: 80%;"}
```{python}
#| eval: false
#| echo: true
#| code-overflow: wrap

from collections import OrderedDict

context_embeddings = []
context_tokens = []
for sentence in sentences:
	tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)
	list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)
  # make ordered dictionary to keep track of the position of each word
	tokens = OrderedDict()
  # loop over tokens in sensitive sentence
	for token in tokenized_text[1:-1]:
		# keep track of position of word and whether it occurs multiple times
		if token in tokens:
			tokens[token] += 1
		else:
		tokens[token] = 1
  	# compute the position of the current token
		token_indices = [i for i, t in enumerate(tokenized_text) if t == token]
		current_index = token_indices[tokens[token]-1]
  	# get the corresponding embedding
		token_vec = list_token_embeddings[current_index]
  	# save values
		context_tokens.append(token)
		context_embeddings.append(token_vec)
		
```
:::

::: aside
A Colab with the full code is [here](https://colab.research.google.com/drive/1ea3zDFrCQFQhkvinaQfdbvXlOhR7hw01?usp=sharing#scrollTo=4wf0epYMLh22)
:::

## Emergence of ML {background-video="img/img_2024_shel/bert_model_01.mp4" background-video-loop="true" background-video-muted="true"}

## Emergence of ML {background-video="img/img_2024_shel/bert_model_02.mp4" background-video-loop="true" background-video-muted="true"}

## Emergence of ML {background-video="img/img_2024_shel/bert_model_03.mp4" background-video-loop="true" background-video-muted="true"}

## Emergence of ML {background-video="img/img_2024_shel/bert_model_04.mp4" background-video-loop="true" background-video-muted="true"}

## Emergence of ML

### LLMs have a broad range of applications…

-   Generation tasks
    -   Chat bots
    -   Content creation
    -   Summarization
    -   Translation
-   Classification tasks
    -   Text classification
    -   Segment classification

## Emergence of ML

### Just as they raise questions regarding…

-   The production of content hallucinations [@ji2023survey; @zhang2023language]
-   Expressions of bias [@santurkar2023opinions]
-   A tendency to repeat back a user’s stated views (“sycophancy”) [@perez2022discovering]

# Implications {background-color="#40666e"}

## Implications

-   A group at CMU was inspired by claims that were circulating when ChatGPT was first introduced. (e.g., "Wow! I asked ChatGPT to write a podcast and it looks pretty good!!!")
-   We wondered what the text it produces looks like when it is repeated. (e.g., "What happens if you ask it to write 100 podcasts?")
-   We gave it the same writing prompt that students are given in in an introductory data science course, generated 100 introduction, and compared those with introductions written by the actual students and introductions that appear in published, data-driven, academic papers.
-   Then, we tagged the data using Biber's [-@biber1991variation] features (which counts things like passives, nominalizations, attributive adjectives, etc.).

## Implications

-   It turns out, machine-authored prose and human-authored prose don’t really look the same in their morphosyntactic and functional features. [@herbold2023large; @markey2024dense]

![Projection of student, published, and ChatGPT-generated writing onto the first two linear discriminants, based on the 67 Biber features.](img/img_2024_shel/lda_scatter.svg)

## Implications

### Human-generated vs. machine-generated text

```{r}
#| echo: false
#| tbl-cap: "Top text features discriminating between human- and machine-generated writing. Color-coded cells show the average z score of each feature. R2 and p-value correspond to one-way ANOVAs predicting each feature with text type."

ld1_tbl <- readr::read_csv("data/data_2024_shel/ld1_tbl.csv")

ld1_tbl <- ld1_tbl |> 
  dplyr::mutate(direction = paste0("Features indicating ", direction, "-generated writing")) |>
  dplyr::mutate(variable = stringr::str_remove(variable, "f_\\d+_")) |>
  dplyr::mutate(variable = stringr::str_replace_all(variable, "_", " ")) |>
  dplyr::filter(direction == "Features indicating human-generated writing") |>
  gt::gt(groupname_col = 'direction') |>
  gt::cols_label(
    variable = gt::md("  "),
    ChatGPT = gt::md("**ChatGPT<br>n:100**"),
    Published = gt::md("**Published<br>n:100**"),
    Student = gt::md("**Student<br>n:100**"),
    r.squared = gt::md("***R*^2^**"),
    p.value = gt::md("***p*-value**")
  ) |> 
  gt::fmt_number(
    columns = dplyr::everything(),
    decimals = 2
  )  |> 
  gt::data_color(
    columns = c(ChatGPT:Student),
    colors = scales::col_numeric(
      palette = c(
        "#FF6666", "white", "#336699"),
      domain = c(pmin(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student), 
                 0, 
                 pmax(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student)))
  ) |>
  gt::tab_style(
    style = list(
      gt::cell_text(style = "italic",
                align = "right")
      ),
    locations = gt::cells_body(
      columns = variable)
    ) |>
  gt::tab_options(quarto.use_bootstrap = TRUE)

ld1_tbl |>
  gt::opt_table_font(weight = "bolder") |>
  gt::as_raw_html()
```

## Implications

### Human-generated vs. machine-generated text

```{r}
#| echo: false
#| tbl-cap: "Top text features discriminating between human- and machine-generated writing. Color-coded cells show the average z score of each feature. R2 and p-value correspond to one-way ANOVAs predicting each feature with text type."

ld1_tbl <- readr::read_csv("data/data_2024_shel/ld1_tbl.csv")

ld1_tbl <- ld1_tbl |> 
  dplyr::mutate(direction = paste0("Features indicating ", direction, "-generated writing")) |>
  dplyr::mutate(variable = stringr::str_remove(variable, "f_\\d+_")) |>
  dplyr::mutate(variable = stringr::str_replace_all(variable, "_", " ")) |>
  dplyr::filter(direction == "Features indicating machine-generated writing") |>
  gt::gt(groupname_col = 'direction') |>
  gt::cols_label(
    variable = gt::md("  "),
    ChatGPT = gt::md("**ChatGPT<br>n:100**"),
    Published = gt::md("**Published<br>n:100**"),
    Student = gt::md("**Student<br>n:100**"),
    r.squared = gt::md("***R*^2^**"),
    p.value = gt::md("***p*-value**")
  ) |> 
  gt::fmt_number(
    columns = dplyr::everything(),
    decimals = 2
  )  |> 
  gt::data_color(
    columns = c(ChatGPT:Student),
    colors = scales::col_numeric(
      palette = c(
        "#FF6666", "white", "#336699"),
      domain = c(pmin(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student), 
                 0, 
                 pmax(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student)))
  ) |>
  gt::tab_style(
    style = list(
      gt::cell_text(style = "italic",
                align = "right")
      ),
    locations = gt::cells_body(
      columns = variable)
    ) |>
  gt::tab_options(quarto.use_bootstrap = TRUE)

ld1_tbl |>
  gt::opt_table_font(weight = "bolder") |>
  gt::as_raw_html()
```

## Implications

::: {style="font-size: 60%;"}
-   ChatGPT, for example, produces a more restricted set of modal verbs -- one that is different from both expert and novice writers.
:::

```{r}
#| echo: false
#| warning: false
#| results: asis
#| tbl-cap: "Frequency of different modal verbs, often modulating the confidence of claims, in the different types of writing."

modal_freq <- readr::read_csv("data/data_2024_shel/modal_freq.csv")

modal_freq <- modal_freq |>
  gt::gt(groupname_col = 'modal_type') |>
  gt::cols_label(
    token =  gt::md("Modal verb"),
    AF_chatgpt =  gt::md("ChatGPT"),
    AF_published =  gt::md("Published"),
    AF_student =  gt::md("Student"),
    RF_chatgpt =  gt::md("ChatGPT"),
    RF_published =  gt::md("Published"),
    RF_student =  gt::md("Student"),
  ) |> 
  gt::tab_spanner(
    label = "Absolute Frequency",
    columns = c(AF_chatgpt, AF_published, AF_student)
  ) |>
  gt::tab_spanner(
    label =  gt::md("Relative Frequency (per 10^5^ words)"),
    columns = c(RF_chatgpt, RF_published, RF_student)
  ) |>
  gt::fmt_number(
    columns = c(RF_chatgpt, RF_published, RF_student),
    decimals = 2
  ) |>
  gt::tab_style(
    style = list(
      gt::cell_text(style = "italic",
                align = "right")
    ),
    locations =  gt::cells_body(
      columns = token,
    )
  )

modal_freq |>
  gt::opt_table_font(weight = "bolder") |>
  gt::tab_options(quarto.disable_processing = TRUE,
                  table.font.size = 14) |>
  gt::as_raw_html()
```

## Implications

![Excerpts of ChatGPT-generated writing with *Noun + Noun* and *Adj. + Noun* sequences underlined in blue and other characteristic features of machine-generated writing marked in red.](img/img_2024_shel/gpt_excerpts_01.png){width="80%"}

::: aside
The LLM has a propensity to condense information into chunked noun phrases, constructed as either *noun* + *noun* or *adjective* + *noun* sequences. Such sequences are sometimes aggregated using the coordinator *and*. On the one hand, these phrases can project a kind of authoritative voice. On the other, their content can range from vague, to ambiguous, to vapid. (What exactly is “a comprehensive dataset encompassing \[..\] healthcare utilization”?)
:::

## Implications

-   Lest you think, "Well, that was a pretty small study..."

::::: columns
::: {.column width="60%"}
![](img/img_2024_shel/hape_workflow.svg){width="100%"}
:::

::: {.column width="40%"}
::: {style="font-size: 80%; padding-top: 25px;"}
We also created an experiment at scale, with 10,000 samples, across 6 text-types, and querying 6 different models.
:::
:::
::::

## Implications

::: {style="font-size: 80%;"}
-   The Human-AI Parallel Corpus in English (HAP-E)
    -   <https://huggingface.co/datasets/browndw/human-ai-parallel-corpus>
:::

```{r}
#| echo: false
#| warning: false
#| tbl-cap: "The composition of the HAP-E corpus."

hape_totals <- readr::read_csv("data/data_2024_shel/hape_totals.csv")

hape_totals |>
  gt::gt(rowname_col = "model_type", groupname_col = "Author") |>
  gt::tab_stubhead(label = "Author") |>
  gt::tab_stub_indent(
    rows = dplyr::everything(),
    indent = 3
  ) |>
  gt::fmt_integer(
    columns = where(is.numeric)
  ) |>
  gt::cols_label(
    .fn = gt::html,
    .process_units = TRUE
  ) |>
    gt::cols_width(
    gt::ends_with("r") ~ gt::px(100)
  ) |>
  gt::grand_summary_rows(
    columns = where(is.numeric),
    fns =  list(label = "TOTALS", id = "totals", fn = "sum"),
    fmt = ~ fmt_integer(.)
  ) |>
  gt::tab_options(table.font.size = 12) |>
  gt::opt_table_font(weight = "bolder") |>
  gt::as_raw_html()
```


## Implications

-   There are clear patterns in the relative frequencies of lemmatized words.

::::: columns
::: {.column width="50%"}
![](img/img_2024_shel/vocab-compare.png){width="75%" fig-align="top"}
:::

::: {.column width="50%"}
::: {style="font-size: 80%; padding-top: 25px;"}
Rates of word use by different LLMs (per 1,000 words) compared to the human use of each word in chunk 2 (log scale). Includes all words used more than once per million words in chunk 2. Words are lemmatized to group together inflected forms. Dashed blue lines indicate the range between 10× more and 10× less than human use. Note that the instruction-tuned models show more variation from the diagonal, indicating more deviation in vocabulary use relative to humans.
:::
:::
::::

## Implications


```{r}
#| echo: false
#| warning: false
#| results: asis
#| tbl-cap: "Most overrepresented words in LLM texts (rates relative to human chunk 2)."

counts <- readr::read_csv("data/data_2024_shel/overrepresented.csv", show_col_types = FALSE)

cols_order <- unlist(lapply(c("GPT-4o", 
                              "GPT-4o Mini", 
                              "Llama 3 70B Instruct",
                              "Llama 3 8B Instruct",
                              "Llama 3 70B",
                              "Llama 3 8B"), 
                            function(x) paste(x, c("Word", "Rate"), sep = "_")))

counts |>
  tidyr::pivot_wider(names_from = "Model", values_from = c(Word, Rate), names_glue = "{Model}_{.value}") |>
  dplyr::select(tidyselect::all_of(c("Rank", cols_order))) |>
  gt::gt(rowname_col = "Rank") |>
    gt::tab_spanner_delim(
      delim = "_"
    ) |>
  gt::opt_table_font(weight = "bolder") |>
  gt::as_raw_html()
```


## Implications

-   As in our earlier study, we tagged the data using Biber's feature set.
-   Binary classification (distinguishing chunk 2 of human writing from the writing produced by a model) using random forests produces high accuracy.

```{r}
#| echo: false
#| warning: false
#| results: asis
#| tbl-cap: "Binary classification accuracy when HAP-E is the training data and predicting COCA-AI Parallel corpus (CAP) data."

result <- readr::read_csv("data/data_2024_shel/rf_accuracy.csv")

result |>
  dplyr::filter(training_data == "HAP-E") |>
  dplyr::mutate(model = stringr::str_replace_all(model, "_", " ")) |>
  dplyr::select(-c(arxiv_accuracy, training_data)) |>
  gt::gt(rowname_col = "model", groupname_col = "model_type") |>
  gt::tab_stubhead(label = "Model") |>
  gt::tab_stub_indent(
    rows = dplyr::everything(),
    indent = 3
  ) |>
  gt::fmt_percent(
    columns = where(is.numeric)
  ) |>
  gt::cols_label(
    model_accuracy = "Acc. (in-sample)",
    pred_accuracy = "Acc. (out-of-sample)"
  ) |>
  gt::opt_table_font(weight = "bolder") |>
  gt::tab_options(quarto.disable_processing = TRUE,
                  table.font.size = 14) |>
  gt::as_raw_html()
```


## Implications

-   In a multi-class random forest, the most important features largely align with the ones we saw in the smaller study.

```{r}
#| echo: false
#| warning: false
#| tbl-cap: "The 10 features with the highest importance."

feature_imp <- readr::read_csv("data/data_2024_shel/feature_importance.csv")

feature_imp <- feature_imp |>
  head(10) |>
  gt::gt() |>
  gt::cols_label(feature = "Feature",
             chunk_1 = "Chunk 1",
             chunk_2 = "Chunk 2",
             chunk_gpt4_mini = "GPT-4o Mini",
             chunk_gpt4 = "GPT-4o",
             llama_3_8b_instruct = "8B",
             llama_3_7b_instruct = "7B",
             llama_3_8b = "8B",
             llama_3_7b = "7B",
            importance = "Importance") |>
  gt::tab_spanner("Human", c(chunk_1, chunk_2)) |>
  gt::tab_spanner("GPT", chunk_gpt4_mini:chunk_gpt4) |>
  gt::tab_spanner("Llama 3 Instruct", llama_3_8b_instruct:llama_3_7b_instruct) |>
  gt::tab_spanner("Llama 3 Base", llama_3_8b:llama_3_7b) |>
  gt::fmt_percent(c(chunk_gpt4_mini:llama_3_7b), decimals = 0) |>
  gt::fmt_number(c(chunk_1, chunk_2, importance), decimals = 1) |>
  gt::tab_header(title = "Features in human- and LLM-written text",
             subtitle = "Rate per 1,000 tokens; LLM rates relative to Chunk 2") |> 
  gt::data_color(
    columns = c(chunk_gpt4_mini:llama_3_7b),
    direction = "row",
    method = "numeric",
    palette = c("#FF6666", "white", "#336699"),
    domain = c(0, 1 ,2),
    na_color = "#336699"
  ) |>
  gt::tab_style(
    style = list(
      gt::cell_text(style = "italic",
                align = "right")
    ),
    locations = gt::cells_body(
      columns = feature,
    )
  )

feature_imp |>
  gt::opt_table_font(weight = "bolder") |>
  gt::as_raw_html()
```


## Implications

-   Carrying our principle component analysis (PCA) on the chunk 2 of the human writing, yields a first principle component (PC1) that strongly resembles [Biber's Demension 1](https://www.uni-bamberg.de/fileadmin/eng-ling/fs/Chapter_21/Index.html?23DimensionsofEnglish.html) (Involved vs. Informational Production).

```{r}
#| echo: false
#| warning: false
#| fig-cap: "Contributions of features to PC1 (based on human chunk 2)."

contrib_df <- readr::read_csv("data/data_2024_shel/variable_contrib.csv", show_col_types = FALSE)

ggplot2::ggplot(contrib_df |> dplyr::filter(above_mean == T), ggplot2::aes(x = reorder(name, contrib), y = contrib)) +
  ggplot2::geom_bar(stat = "identity",
           show.legend = FALSE, fill = "#440154") +
  ggplot2::xlab("") +
  ggplot2::ylab("Contribution (%)") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

```

## Implications

![Projection of human chunk 2 writing onto the first princple component, based on the 67 Biber features.](img/img_2024_shel/pca_human.svg)

## Implications

![Projection of human chunk 2 writing and Llama Base onto the first princple component.](img/img_2024_shel/pca_base.svg)

## Implications

![Projection of human chunk 2 writing and Llama Instruct onto the first princple component.](img/img_2024_shel/pca_instruct.svg)

## Implications

![Projection of human chunk 2 writing and GPT 4o onto the first princple component.](img/img_2024_shel/pca_gpt.svg)

## Implications

### Key takeaways

::: {style="font-size: 80%;"}
1.  ChatGPT produces sentences that are [more informationally dense]{style="background-color: #f5b2c6; opacity: 0.75;"} than that of human writing, but the information density is expressed using repetitive grammatical patterns.
2.  The tendency toward greater information density appears to be [magnified by reinforcement learning]{style="background-color: #f5b2c6; opacity: 0.75;"} (also called instruction tuning), though the reasons for that magnification are unclear.
3.  Machine-generated writing demonstrates [less modulation of stress or confidence]{style="background-color: #f5b2c6; opacity: 0.75;"}.
5.  Broadly, LLMs are [not as grammatically nimble]{style="background-color: #f5b2c6; opacity: 0.75;"} as human writers.
5.  LLM-generated text generally produces academic prose [expressing limited engagement with core, disciplinary concepts]{style="background-color: #f5b2c6; opacity: 0.75;"} either in the way that experts do or in the way that novice students do. (Though LLMs can arrive at something more like expert writing with iterative prompting.)
:::

## Works Cited
